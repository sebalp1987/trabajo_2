from sklearn import linear_model
import statsmodels.stats.stattools
import matplotlib.pyplot as plot
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import pandas as pd
from statsmodels.stats.diagnostic import acorr_ljungbox
from pandas.plotting import autocorrelation_plot


def dw_test(x, y, plot_show=True):
    """
    The test statistic is approximately equal to 2*(1-r) where r is the sample autocorrelation of the residuals.
    Thus, for r == 0, indicating no serial correlation, the test statistic equals 2. This statistic will always be
    between 0 and 4. The closer to 0 the statistic, the more evidence for positive serial correlation. The closer to 4,
    the more evidence for negative serial correlation.

    A partial autocorrelation is a summary of the relationship between an observation in a time series with observations
    at prior time steps with the relationships of intervening observations removed.

    The partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations
    due to the terms at shorter lags.

    Autoregression Intuition
    Consider a time series that was generated by an autoregression (AR) process with a lag of k.

    We know that the ACF describes the autocorrelation between an observation and another observation at a prior time
    step that includes direct and indirect dependence information.

    This means we would expect the ACF for the AR(k) time series to be strong to a lag of k and the inertia of that
    relationship would carry on to subsequent lag values, trailing off at some point as the effect was weakened.

    We know that the PACF only describes the direct relationship between an observation and its lag. This would suggest
    that there would be no correlation for lag values beyond k.

    This is exactly the expectation of the ACF and PACF plots for an AR(k) process.

    Moving Average Intuition
    Consider a time series that was generated by a moving average (MA) process with a lag of k.

    Remember that the moving average process is an autoregression model of the time series of residual errors from prior
    predictions. Another way to think about the moving average model is that it corrects future forecasts based on
    errors made on recent forecasts.

    We would expect the ACF for the MA(k) process to show a strong correlation with recent values up to the lag of k,
    then a sharp decline to low or no correlation. By definition, this is how the process was generated.

    For the PACF, we would expect the plot to show a strong relationship to the lag and a trailing off of correlation
    from the lag onwards.

    Again, this is exactly the expectation of the ACF and PACF plots for an MA(k) process.
    """
    if plot_show:
        fig = plot.figure(figsize=(12, 8))
        ax1 = fig.add_subplot(211)
        fig = plot_acf(y, lags=40, ax=ax1)
        ax2 = fig.add_subplot(212)
        fig = plot_pacf(y, lags=40, ax=ax2)
        plot.show()
        plot.close()
    x = x.values
    y = y.values
    file_model = linear_model.LinearRegression()
    file_model.fit(x, y)
    prediction = file_model.predict(x)
    error = []
    for i in range(len(y)):
        error.append(y[i] - prediction[i])
    print('dw test', statsmodels.stats.stattools.durbin_watson(error, axis=0))
    plot.scatter(range(len(y)), error, alpha=0.5, s=120)
    if plot_show:
        plot.xlabel('X Values')
        plot.ylabel('Y Values')
        plot.show()


def test_stationarity(timeseries, plot_show=False, lags=7):
    """
    # Perform Dickey-Fuller test:
    if the ‘Test Statistic’ is greater than the ‘Critical Value’ than the time series is stationary.

    Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is
    non-stationary. It has some time dependent structure.
    Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root,
    meaning it is stationary. It does not have time-dependent structure.
    p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.
    p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.
    """

    # Determing rolling statistics
    rolmean = timeseries.rolling(lags).mean()
    rolstd = timeseries.rolling(lags).std()

    # Plot rolling statistics:
    if plot_show:
        fig = plot.figure(figsize=(12, 8))
        orig = plot.plot(timeseries, color='lightseagreen', label='Original')
        mean = plot.plot(rolmean, color='lightcoral', label='Rolling Mean')
        std = plot.plot(rolstd, color='mediumseagreen', label='Rolling Std')
        plot.legend(loc='best')
        plot.title('Rolling Mean & Standard Deviation')
        plot.show()
        plot.close()

    dftest = adfuller(timeseries, autolag='AIC', maxlag=7)
    dfoutput = pd.Series(dftest[0:4],
                             index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])
    for key, value in dftest[4].items():
        dfoutput['Critical Value (%s)' % key] = value
    print(dfoutput)
    return dfoutput[0], dfoutput[4]


def ljung_box_test(x, y):
    file_model = linear_model.LinearRegression()
    file_model.fit(x, y)
    prediction = file_model.predict(x)
    error = []
    for i in range(len(y)):
        error.append(y[i] - prediction[i])
    nobs = len(x[0])
    print('Ljung-Box Test', statsmodels.tsa.stattools.q_stat(error, nobs, type='ljungbox'))


def serial_correlation(variable, plot_show=True):
    """
    Since no p-value is below .05, both tests agree that you can not reject the null of no auto-correlation between the
    series and each of it's first XX lags with > 95% confidence level.
    """
    if plot_show:
        autocorrelation_plot(variable)
        plot.show()
        plot.close()
    # https://robjhyndman.com/hyndsight/ljung-box-test/
    lags = min(14, round(len(variable)/5))
    qljb, pvalue = acorr_ljungbox(variable, lags=lags)
    print(qljb)
    print(pvalue)

def get_residuals(X, Y):
    error = []
    fileModel = linear_model.LinearRegression()
    fileModel.fit(X, Y)
    prediction = fileModel.predict(X)

    for i in range(len(Y)):
        error.append(Y[i] - prediction[i])

    error = pd.DataFrame(error, columns=['error'])
    return error
